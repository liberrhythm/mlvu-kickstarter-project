{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# important basic imports\n",
    "\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import string\n",
    "# !pip install nltk\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as clrs\n",
    "%matplotlib inline\n",
    "\n",
    "# if you get errors about installation or downloading, uncomment the above commented lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# relevant imports\n",
    "\n",
    "# data prep\n",
    "from sklearn import preprocessing\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from scipy import stats\n",
    "\n",
    "# nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# import matplotlib\n",
    "from matplotlib.pyplot import scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "df = pd.read_csv(\"./ks-projects-201801.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keep only campaigns that were successful or failed\n",
    "df['state_string'] = [str(state) for state in df['state']]\n",
    "df = df[(df.state_string == 'successful') | (df.state_string == 'failed')]\n",
    "df = df.drop(['state_string'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill nan/missing values with correct values\n",
    "df['usd pledged'].fillna(df['usd_pledged_real'], inplace=True)\n",
    "df['name'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make new duration and cross-product feature for testing in model\n",
    "df['deadline'] = pd.to_datetime(df['deadline'])\n",
    "df['launched'] = pd.to_datetime(df['launched'])\n",
    "df['duration_days'] = (df['deadline'] - df['launched']).astype('timedelta64[D]')\n",
    "df['goal*days'] = df['usd_goal_real']*df['duration_days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove outliers - do or not do?\n",
    "# df1 = df[(np.abs(stats.zscore(df[['usd_goal_real']]) < 3))]\n",
    "# df2 = df[(np.abs(stats.zscore(df[['duration_days']]) < 3))]\n",
    "\n",
    "# df = pd.merge(df1, df2, how='inner')\n",
    "# new_num_rows = len(df)\n",
    "# pct_data_kept = new_num_rows/num_rows\n",
    "# print(pct_data_kept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert dataframe columns with dtype object to categories\n",
    "df['state'] = df['state'].astype('category')\n",
    "df['main_category'] = df['main_category'].astype('category')\n",
    "df['category'] = df['category'].astype('category')\n",
    "df['currency'] = df['currency'].astype('category')\n",
    "df['country'] = df['country'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# format name column and add word count/name sentiment features\n",
    "df['name'] = [str(name) for name in df['name']]\n",
    "df['name_word_count'] = [len(name.split(\" \")) for name in df['name']]\n",
    "df['name_sentiment'] = [TextBlob(name).sentiment.polarity for name in df['name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encoding for main category and country columns\n",
    "main_categories_cols = pd.get_dummies(df['main_category'], prefix = 'main_category')\n",
    "country_cols = pd.get_dummies(df['country'], prefix = 'country')\n",
    "df = pd.concat([df, main_categories_cols, country_cols], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('all_features_no_tfidf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_features = list(df)\n",
    "\n",
    "# segment features into original, unnecessary, unknown, and BASE (what we want to use for our models)\n",
    "unknown_features = ['pledged','backers','usd pledged','usd_pledged_real','currency']\n",
    "unnecessary_features = ['ID','deadline','launched','category','goal','main_category','country']\n",
    "fts = ['usd_goal_real','duration_days','goal*days']\n",
    "base_features = [feat for feat in all_features if feat not in unknown_features and feat not in unnecessary_features]\n",
    "df1 = df[base_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training/validation/testing 60/20/20 split\n",
    "y = df1['state'] # define target variable\n",
    "x = df1.drop('state', axis=1)  \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize numeric data using min_max normalization\n",
    "# x_train[fts] = (x_train[fts]-x_train[fts].min())/(x_train[fts].max()-x_train[fts].min())\n",
    "# x_val[fts] = (x_val[fts]-x_val[fts].min())/(x_val[fts].max()-x_val[fts].min())\n",
    "# x_test[fts] = (x_test[fts]-x_test[fts].min())/(x_test[fts].max()-x_test[fts].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# standardize numeric data\n",
    "x_train[fts] = (x_train[fts]-x_train[fts].mean())/(x_train[fts].std())\n",
    "x_val[fts] = (x_val[fts]-x_val[fts].mean())/(x_val[fts].std())\n",
    "x_test[fts] = (x_test[fts]-x_test[fts].mean())/(x_test[fts].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(199005, 44)\n",
      "(66335, 44)\n",
      "(66335, 44)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup preprocessing and tokenization for tfidf vectorizer\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "punctuation = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def preprocessing(name):\n",
    "    return name.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# tokenize the name and lemmatize its tokens\n",
    "def tokenizing(name):\n",
    "    return [lemmatizer.lemmatize(t) for t in word_tokenize(name) if t not in stopwords and len(t)>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create tfidf vectorizer and generate word vectors\n",
    "vectorizer = TfidfVectorizer(preprocessor=preprocessing,\n",
    "                             tokenizer=tokenizing,\n",
    "                             strip_accents='ascii',\n",
    "                             analyzer='word',\n",
    "                             sublinear_tf=True,\n",
    "                             min_df=100,\n",
    "                             ngram_range=(1,2))\n",
    "\n",
    "tfidf = vectorizer.fit_transform(x_train['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(199005, 1450)\n"
     ]
    }
   ],
   "source": [
    "# tfidf for training data\n",
    "tfidf_words = [(\"tfidf_\" + word) for word in vectorizer.get_feature_names()]\n",
    "x_train_tfidf = pd.DataFrame(tfidf.toarray(), columns=tfidf_words)\n",
    "\n",
    "x_train.reset_index(drop=True, inplace=True)\n",
    "x_train_tfidf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "x_train_new = pd.concat([x_train, x_train_tfidf], axis=1)\n",
    "print(x_train_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66335, 1450)\n"
     ]
    }
   ],
   "source": [
    "# tfidf for validation data\n",
    "tfidf_val = vectorizer.transform(x_val['name'])\n",
    "x_val_tfidf = pd.DataFrame(tfidf_val.toarray(), columns=tfidf_words)\n",
    "\n",
    "x_val.reset_index(drop=True, inplace=True)\n",
    "x_val_tfidf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "x_val_new = pd.concat([x_val, x_val_tfidf], axis=1)\n",
    "print(x_val_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66335, 1450)\n"
     ]
    }
   ],
   "source": [
    "# tfidf for testing data\n",
    "tfidf_test = vectorizer.transform(x_test['name'])\n",
    "x_test_tfidf = pd.DataFrame(tfidf_test.toarray(), columns=tfidf_words)\n",
    "\n",
    "x_test.reset_index(drop=True, inplace=True)\n",
    "x_test_tfidf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "x_test_new = pd.concat([x_test, x_test_tfidf], axis=1)\n",
    "print(x_test_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_val.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove name column\n",
    "x_train_new = x_train_new.drop(['name'], axis=1)\n",
    "x_val_new = x_val_new.drop(['name'], axis=1)\n",
    "x_test_new = x_test_new.drop(['name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save training data\n",
    "x_train_new.to_csv('x_train.csv', index=False)\n",
    "y_train.to_csv('y_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save validation data\n",
    "x_val_new.to_csv('x_val.csv', index=False)\n",
    "y_val.to_csv('y_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save testing data\n",
    "x_test_new.to_csv('x_test.csv', index=False)\n",
    "y_test.to_csv('y_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199005, 1449)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# below is code that I wrote that we're not using anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "# from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "# main_categories = x_train['main_category']\n",
    "# print(np.unique(main_categories))\n",
    "# integer encode\n",
    "# label_encoder = LabelEncoder()\n",
    "# integer_encoded = label_encoder.fit_transform(main_categories)\n",
    "# print(len(integer_encoded))\n",
    "# binary encode\n",
    "# onehot_encoder = OneHotEncoder(sparse=False)\n",
    "# integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "# onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "# print(onehot_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn names into strings, remove punctuation, and tokenize \n",
    "# table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "# lowercase_names = df['name'].astype(str).str.lower()\n",
    "# token_lists = [nltk.word_tokenize(name.translate(table)) for name in lowercase_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "# def remove_stopwords(token_list):\n",
    "#     return [token for token in token_list if not token in stopwords]\n",
    "\n",
    "# df['name_tokens'] = [remove_stopwords(token_list) for token_list in token_lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create array of all words\n",
    "# words = [word for word_list in df['name_tokens'] for word in word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word to vec\n",
    "# from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "# glove_input_file = 'glove.6B.100d.txt'\n",
    "# word2vec_output_file = 'glove.6B.100d.txt.word2vec'\n",
    "# glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
